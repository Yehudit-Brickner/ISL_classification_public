{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run this notebook you will need your own data set.\n",
    "\n",
    "### It will need to have sub folders with the correct names for each class, this is important for the functions loadtestdata and loadDataGetDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.models import ResNet50_Weights\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a class for the classifier\n",
    "\n",
    "Defining all of the function needed\n",
    "<br> 1- loadDataGetDF, this function loads the data and splits into train, validation and test.\n",
    "<br> 2- loadtestdata, this function can be used to load data to test the model with.\n",
    "<br> 3- setFeatures, this function sets up the features for the model.\n",
    "<br> 4- trainModel, this function trains the model.\n",
    "<br> 5- testmodel, this function tests the accuracy of a model. \n",
    "<br> - cofussion_matrix, this function presents a pretty confusion matrix.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifier():\n",
    "    def __init__(self,FC,trainpath=None,testpath=None):\n",
    "        self.FC=FC\n",
    "        self.train_path=trainpath\n",
    "        self.test_path=testpath\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        self.train_transforms = A.Compose([\n",
    "            # A.GaussNoise(var_limit=(10.0, 50.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n",
    "            # A.Blur(blur_limit=7, always_apply=False, p=0.5),\n",
    "            # A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "            # A.Rotate(15),\n",
    "            # A.ISONoise(color_shift=(0.01, 0.05), intensity=(0.1, 0.5), always_apply=False, p=0.5),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.test_transforms = A.Compose([\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        self.model = models.resnet50(weights= ResNet50_Weights.DEFAULT).to(self.device)\n",
    "        self.optimizer=None\n",
    "        self.criterion=None\n",
    "        self.setFeatures()\n",
    "        \n",
    "    def loadDataGetDF(self):\n",
    "        classes2=['0-aleph', '1-bet', '2-gimel', '3-daled', '4-hey', '5-vav',\n",
    "        '6-chet', '7-tet', '8-yud', '9-caph','10-lamed', '11-mem', '12-non', '13-samech',\n",
    "        '14-ein','15-pay', '16-sadik', '17-koph', '18-raish', '19-shin',  '20-taph', ]\n",
    "        train_images = []\n",
    "        train_labels = []\n",
    "        encoded_labels = []\n",
    "        num=0\n",
    "        for c in classes2:\n",
    "            i=0\n",
    "            flist = os.listdir(self.train_path + '/' + c)\n",
    "            for file in flist:\n",
    "                file_path = os.path.join(self.train_path, c, file)\n",
    "                train_images.append(file_path)\n",
    "                train_labels.append(c)\n",
    "                encoded_labels.append(num)\n",
    "            num+=1\n",
    "\n",
    "        train_images = pd.Series(train_images, name='file_paths')\n",
    "        train_labels = pd.Series(train_labels, name='labels')\n",
    "        encoded_labels = pd.Series(encoded_labels, name='encoded_labels')\n",
    "        train_df = pd.DataFrame(pd.concat([train_images, train_labels, encoded_labels], axis=1))\n",
    "        train_df = train_df.sample(frac=1)\n",
    "\n",
    "        # split the data to into train test and validation\n",
    "        train_df, valid_df = train_test_split(train_df, train_size=0.8, random_state=0)\n",
    "        test_df, valid_df = train_test_split(valid_df, train_size=0.5, random_state=0)\n",
    "\n",
    "        print(\"train size: \", len(train_df))\n",
    "        print(\"val size: \", len(valid_df))\n",
    "        print(\"test size: \", len(test_df))\n",
    "\n",
    "        return (train_df, valid_df,test_df)\n",
    "    \n",
    "\n",
    "    def loadtestdata(self):\n",
    "   \n",
    "        classes2 = ['0-aleph', '1-bet', '2-gimel', '3-daled', '4-hey', '5-vav',\n",
    "                    '6-chet', '7-tet', '8-yud', '9-caph', '10-lamed', '11-mem', '12-non', '13-samech',\n",
    "                    '14-ein', '15-pay', '16-sadik', '17-koph', '18-raish', '19-shin', '20-taph', ]\n",
    "        test_images = []\n",
    "        test_labels = []\n",
    "        test_encoded_labels = []\n",
    "        num = 0\n",
    "        for c in classes2:\n",
    "            flist = os.listdir(self.test_path + '/' + c)\n",
    "            for file in flist:\n",
    "                file_path = os.path.join(self.test_path, c, file)\n",
    "                test_images.append(file_path)\n",
    "                test_labels.append(c)\n",
    "                test_encoded_labels.append(num)\n",
    "            num += 1\n",
    "        test_images = pd.Series(test_images, name='file_paths')\n",
    "        test_labels = pd.Series(test_labels, name='labels')\n",
    "        test_encoded_labels = pd.Series(test_encoded_labels, name='encoded_labels')\n",
    "        test_df = pd.DataFrame(pd.concat([test_images, test_labels, test_encoded_labels], axis=1))\n",
    "\n",
    "        return test_df\n",
    "\n",
    "    def setFeatures(self):\n",
    "        if self.FC==False:\n",
    "            In_features = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(In_features,21)\n",
    "        else:\n",
    "            In_features = self.model.fc.in_features\n",
    "            self.model.fc=nn.Sequential( \n",
    "                                    nn.Linear(In_features,1050),\n",
    "                                    nn.Linear(1050,210),\n",
    "                                    nn.Linear(210,21))\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # Define the loss function and optimizer\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(),lr=0.0001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = self.criterion.to(self.device)\n",
    "             \n",
    "\n",
    "    def trainModel(self,epochs, train_loader, valid_loader, name):\n",
    "        total_train_loss = []\n",
    "        total_valid_loss = []\n",
    "        validationAccuracy = []\n",
    "        trainAccuracy = []\n",
    "        best_valid_loss = np.Inf\n",
    "        \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print('Epoch: ', epoch + 1)\n",
    "            train_loss = []\n",
    "            valid_loss = []\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            valid_correct = 0\n",
    "            valid_total = 0\n",
    "            for image, target in train_loader:\n",
    "                self.model.train()\n",
    "                target = target.type(torch.LongTensor)\n",
    "                image = image.float()\n",
    "                image, target = image.to(self.device), target.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(image)\n",
    "                loss = self.criterion(output, target)\n",
    "                train_loss.append(loss.item())\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                train_total += target.size(0)\n",
    "                train_correct += (predicted == target).sum().item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "            for image, target in valid_loader:\n",
    "                with torch.no_grad():\n",
    "                    self.model.eval()\n",
    "                    target = target.type(torch.LongTensor)\n",
    "                    image = image.float()\n",
    "                    image, target = image.to(self.device), target.to(self.device)\n",
    "\n",
    "                    output = self.model(image)\n",
    "                    loss = self.criterion(output, target)\n",
    "                    valid_loss.append(loss.item())\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    valid_total += target.size(0)\n",
    "                    valid_correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_train_loss = np.mean(train_loss)\n",
    "            epoch_valid_loss = np.mean(valid_loss)\n",
    "            print(\n",
    "                f'Epoch {epoch + 1}, train loss: {epoch_train_loss:.4f}, valid loss: {epoch_valid_loss:.4f}, train accuracy: {(100 * train_correct / train_total):.4f}%, valid accuracy: {(100 * valid_correct / valid_total):.4f}%')\n",
    "            trainAccuracy.append(100 * train_correct / train_total)\n",
    "            validationAccuracy.append(100 * valid_correct / valid_total)\n",
    "            if epoch_valid_loss < best_valid_loss:\n",
    "        \n",
    "                torch.save(self.model.state_dict(), name)\n",
    "                print('Model improved. Saving model.')\n",
    "                best_valid_loss = epoch_valid_loss\n",
    "\n",
    "            total_train_loss.append(epoch_train_loss)\n",
    "            total_valid_loss.append(epoch_valid_loss)\n",
    "\n",
    "        plt.plot(total_train_loss, 'b')\n",
    "        plt.plot(total_valid_loss, 'r')\n",
    "        plt.legend(['train loss', 'validation lost'])\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(trainAccuracy, 'b')\n",
    "        plt.plot(validationAccuracy, 'r')\n",
    "        plt.legend(['train accuracy', 'validation accuracy'])\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def testmodel(self,test_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        testPred = []\n",
    "        testActual = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            for image, target in test_loader:\n",
    "                testActual.append(target.tolist())\n",
    "                target = target.type(torch.LongTensor)\n",
    "                image = image.float()\n",
    "                image, target = image.to(self.device), target.to(self.device)\n",
    "\n",
    "                output = self.model(image)\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                testPred.append(predicted.tolist())\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "        print('Test Accuracy: %d %%' % (100 * correct / total))\n",
    "        return testActual,testPred\n",
    "\n",
    "\n",
    "    def confussion_matrix(self,cf_matrix):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        x_axis_labels=['א','ב','ג','ד','ה','ו','ח','ט','י','כ','ל','מ','נ','ס','ע','פ','צ','ק','ר','ש','ת']\n",
    "        y_axis_labels=['א','ב','ג','ד','ה','ו','ח','ט','י','כ','ל','מ','נ','ס','ע','פ','צ','ק','ר','ש','ת']\n",
    "        sns.heatmap(cf_matrix, annot=True, fmt='',xticklabels=x_axis_labels, yticklabels=y_axis_labels, cmap='Greens')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset from the dataframe\n",
    "# this class gets a df and has the function to give us items(tensors) using the transforms\n",
    "class ISLAlphabet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transform=transforms.Compose([transforms.ToTensor()])):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        length = len(self.df)\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.df.iloc[idx, 0]\n",
    "        label = self.df.iloc[idx, 2]\n",
    "        label = torch.tensor(label)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = image.resize((224,224))\n",
    "        img = np.array(image)\n",
    "        image = self.transform(image=img)[\"image\"]\n",
    "        return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to train the model and get the results \n",
    "(the data here is the big with no augmentation, to add augentations you need to change the train_transforms in the init function for my classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of MyClassifer with train data\n",
    "mc = MyClassifier (False, trainpath=\"file_path\",testpath=None)\n",
    "# get the data frames\n",
    "train_df, valid_df,test_df = mc.loadDataGetDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create the dataset classes\n",
    "train_dataset = ISLAlphabet(df=train_df, transform=mc.train_transforms)\n",
    "valid_dataset = ISLAlphabet(df=valid_df, transform=mc.test_transforms)\n",
    "test_dataset = ISLAlphabet(df=test_df, transform=mc.test_transforms)\n",
    "\n",
    "# tell the loaders which dataset class to use, and how many instances to load at a time\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "mc.trainModel(10, train_loader = train_loader, valid_loader = valid_loader, name=\"Isl_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "\n",
    "mc.model.load_state_dict(torch.load('Isl_model.pt'))\n",
    "mc.model = mc.model.to(mc.device)\n",
    "\n",
    "# test the model and create a confusion matrix\n",
    "testActual,testPred = mc.testmodel(test_loader)\n",
    "mc.confussion_matrix(confusion_matrix(testActual,testPred))\n",
    "\n",
    "# calc and print scores\n",
    "ac=accuracy_score(testActual, testPred)\n",
    "pre=precision_score(testActual, testPred, average='weighted')\n",
    "rec=recall_score(testActual, testPred, average='weighted')\n",
    "f1=f1_score(testActual, testPred, average='weighted')\n",
    "print('Accuracy score : ', ac)\n",
    "print('Precision score : ', pre)\n",
    "print('Recall score : ', rec)\n",
    "print('F1 score : ', f1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a test data set of big images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of MyClassifer with train data\n",
    "mc = MyClassifier (False, trainpath=None ,testpath=\"file_path\")\n",
    "# get the data frames\n",
    "test_df = mc.loadtestdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset classes\n",
    "test_dataset = ISLAlphabet(df=test_df, transform=mc.test_transforms)\n",
    "\n",
    "# tell the loaders which dataset class to use, and how many instances to load at a time\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model on the big images\n",
    "mc.model.load_state_dict(torch.load('ISL_big_images_best.pt'))\n",
    "mc.model = mc.model.to(mc.device)\n",
    "\n",
    "# test the model and create a confusion matrix\n",
    "testActual,testPred = mc.testmodel(test_loader)\n",
    "mc.confussion_matrix(confusion_matrix(testActual,testPred))\n",
    "\n",
    "# calc and print scores\n",
    "ac=accuracy_score(testActual, testPred)\n",
    "pre=precision_score(testActual, testPred, average='weighted')\n",
    "rec=recall_score(testActual, testPred, average='weighted')\n",
    "f1=f1_score(testActual, testPred, average='weighted')\n",
    "print('Accuracy score : ', ac)\n",
    "print('Precision score : ', pre)\n",
    "print('Recall score : ', rec)\n",
    "print('F1 score : ', f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeat with the cropped data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of MyClassifer with train data\n",
    "mc = MyClassifier (True, trainpath=None ,testpath=\"file_path\")\n",
    "# get the data frames\n",
    "test_df = mc.loadtestdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset classes\n",
    "test_dataset = ISLAlphabet(df=test_df, transform=mc.test_transforms)\n",
    "\n",
    "# tell the loaders which dataset class to use, and how many instances to load at a time\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model on the big images\n",
    "mc.model.load_state_dict(torch.load('ISL_cropped_images_best.pt'))\n",
    "mc.model = mc.model.to(mc.device)\n",
    "\n",
    "# test the model and create a confusion matrix\n",
    "testActual,testPred = mc.testmodel(test_loader)\n",
    "mc.confussion_matrix(confusion_matrix(testActual,testPred))\n",
    "\n",
    "# calc and print scores\n",
    "ac=accuracy_score(testActual, testPred)\n",
    "pre=precision_score(testActual, testPred, average='weighted')\n",
    "rec=recall_score(testActual, testPred, average='weighted')\n",
    "f1=f1_score(testActual, testPred, average='weighted')\n",
    "print('Accuracy score : ', ac)\n",
    "print('Precision score : ', pre)\n",
    "print('Recall score : ', rec)\n",
    "print('F1 score : ', f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
